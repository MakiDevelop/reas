# ğŸ¤– çˆ¬èŸ²ä»£ç†ç³»çµ± (Crawler Agents) è¨­è¨ˆæ–‡æª”

## ğŸ“‹ ç›®éŒ„
- [å°ˆæ¡ˆæ¦‚è¿°](#å°ˆæ¡ˆæ¦‚è¿°)
- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [ç³»çµ±æ¶æ§‹](#ç³»çµ±æ¶æ§‹)
- [æŠ€è¡“æ£§](#æŠ€è¡“æ£§)
- [è¨­è¨ˆæ¨¡å¼](#è¨­è¨ˆæ¨¡å¼)
- [é—œéµçµ„ä»¶](#é—œéµçµ„ä»¶)
- [æœ€ä½³å¯¦è¸](#æœ€ä½³å¯¦è¸)
- [æ“´å±•æŒ‡å—](#æ“´å±•æŒ‡å—)
- [ç¶“é©—ç¸½çµ](#ç¶“é©—ç¸½çµ)

---

## å°ˆæ¡ˆæ¦‚è¿°

### ğŸ¯ å°ˆæ¡ˆç›®çš„
å»ºç«‹ä¸€å€‹**å¤šä¾†æºæˆ¿åœ°ç”¢æ–°èçˆ¬èŸ²ç³»çµ±**ï¼Œè‡ªå‹•åŒ–æ”¶é›†ã€è§£æã€å„²å­˜ä¾†è‡ªå¤šå€‹æ–°èç¶²ç«™çš„æˆ¿åœ°ç”¢ç›¸é—œæ–‡ç« ã€‚

### ğŸ¢ æ¥­å‹™éœ€æ±‚
- **å¤šä¾†æºæ•´åˆ**ï¼šæ”¯æ´ 8 å€‹æ–°èä¾†æºï¼ˆå°ç£ã€é¦¬ä¾†è¥¿äºã€é¦™æ¸¯ï¼‰
- **å®šæ™‚çˆ¬å–**ï¼šæ¯å¤©è‡ªå‹•çˆ¬å– 4 æ¬¡ï¼ˆ8am, 12pm, 4pm, 8pmï¼‰
- **è³‡æ–™ç®¡ç†**ï¼šçµ±ä¸€å„²å­˜ã€å»é‡ã€åˆ†é¡ã€æŸ¥è©¢
- **Web ä»‹é¢**ï¼šæä¾›æœå°‹ã€ç€è¦½ã€åŒ¯å‡ºåŠŸèƒ½

### ğŸ¨ è¨­è¨ˆç†å¿µ
1. **ä»£ç†æ¨¡å¼ï¼ˆAgent Patternï¼‰**ï¼šæ¯å€‹æ–°èä¾†æºæ˜¯ç¨ç«‹çš„ã€Œä»£ç†ã€
2. **å¯æ“´å±•æ€§**ï¼šè¼•é¬†æ·»åŠ æ–°çš„æ–°èä¾†æº
3. **å®¹éŒ¯æ€§**ï¼šå–®å€‹ä»£ç†å¤±æ•—ä¸å½±éŸ¿æ•´é«”ç³»çµ±
4. **è‡ªå‹•åŒ–**ï¼šæœ€å°åŒ–äººå·¥ä»‹å…¥

---

## æ ¸å¿ƒæ¦‚å¿µ

### ä»€éº¼æ˜¯ã€ŒAgentã€ï¼ˆä»£ç†ï¼‰ï¼Ÿ

åœ¨æœ¬å°ˆæ¡ˆä¸­ï¼Œ**Agent = Crawlerï¼ˆçˆ¬èŸ²ï¼‰**ï¼Œæ¯å€‹ Agent è² è²¬ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Crawler Agent               â”‚
â”‚                                     â”‚
â”‚  1. é€£æ¥åˆ°ç‰¹å®šæ–°èç¶²ç«™              â”‚
â”‚  2. è§£æç¶²ç«™ç‰¹å®šçš„ HTML çµæ§‹        â”‚
â”‚  3. æå–æ–‡ç« è³‡æ–™ï¼ˆæ¨™é¡Œã€å…§å®¹ç­‰ï¼‰    â”‚
â”‚  4. è™•ç†éŒ¯èª¤å’Œé‡è©¦                  â”‚
â”‚  5. è¿”å›çµæ§‹åŒ–è³‡æ–™                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent çš„ç‰¹æ€§

- âœ… **è‡ªä¸»æ€§**ï¼šç¨ç«‹åŸ·è¡Œï¼Œä¸ä¾è³´å…¶ä»– Agent
- âœ… **å°ˆæ¥­åŒ–**ï¼šå°ˆé–€è™•ç†ç‰¹å®šç¶²ç«™çš„çµæ§‹
- âœ… **æ¨™æº–åŒ–**ï¼šéµå¾ªçµ±ä¸€çš„ä»‹é¢è¦ç¯„
- âœ… **å®¹éŒ¯æ€§**ï¼šå¤±æ•—ä¸å½±éŸ¿å…¶ä»– Agent

---

## ç³»çµ±æ¶æ§‹

### æ•´é«”æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FastAPI Application                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Web UI     â”‚  â”‚ REST API   â”‚  â”‚ Scheduler          â”‚   â”‚
â”‚  â”‚            â”‚  â”‚            â”‚  â”‚ (APScheduler)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Crawler Orchestrator                â”‚
        â”‚   (ä¸¦è¡Œ/ä¸²è¡ŒåŸ·è¡Œæ§åˆ¶)                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                           â”‚
        â–¼                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BaseCrawler  â”‚ (æŠ½è±¡åŸºé¡)                â”‚ DB Utils     â”‚
â”‚              â”‚                           â”‚              â”‚
â”‚ - å…±ç”¨é‚è¼¯   â”‚                           â”‚ - æ‰¹æ¬¡æ“ä½œ   â”‚
â”‚ - é‡è©¦æ©Ÿåˆ¶   â”‚                           â”‚ - Upsert     â”‚
â”‚ - æ—¥æœŸè§£æ   â”‚                           â”‚ - æ¸…ç†       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼         â–¼         â–¼         â–¼         â–¼        â–¼
    â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”  ...
    â”‚ LTN â”‚   â”‚ UDN â”‚   â”‚Appleâ”‚   â”‚ETdayâ”‚   â”‚Edge â”‚
    â”‚Agentâ”‚   â”‚Agentâ”‚   â”‚Agentâ”‚   â”‚Agentâ”‚   â”‚Agentâ”‚
    â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜
        â”‚         â”‚         â”‚         â”‚         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  PostgreSQL DB   â”‚
                â”‚                  â”‚
                â”‚  articles table  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è³‡æ–™æµç¨‹

```
ä½¿ç”¨è€…/æ’ç¨‹å™¨è§¸ç™¼
       â”‚
       â–¼
é¸æ“‡è¦åŸ·è¡Œçš„ Agent(s)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ä¸¦è¡Œ/ä¸²è¡ŒåŸ·è¡Œ    â”‚
â”‚                  â”‚
â”‚ Agent 1 â”€â”€â”€â”€â”   â”‚
â”‚ Agent 2 â”€â”€â”€â”€â”¤   â”‚
â”‚ Agent 3 â”€â”€â”€â”€â”¤   â”‚
â”‚    ...      â”‚   â”‚
â”‚ Agent N â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
æ¯å€‹ Agent ç¨ç«‹åŸ·è¡Œï¼š
  1. è¨ªå•ç¶²ç«™
  2. è§£æåˆ—è¡¨é 
  3. çˆ¬å–æ–‡ç« å…§å®¹
  4. æ¸…ç†å’Œæ ¼å¼åŒ–
       â”‚
       â–¼
è³‡æ–™æ‰¹æ¬¡å¯«å…¥è³‡æ–™åº«
  (Upsert: æ–°å¢æˆ–æ›´æ–°)
       â”‚
       â–¼
è¿”å›åŸ·è¡Œçµæœ
  (æˆåŠŸ/å¤±æ•—çµ±è¨ˆ)
```

---

## æŠ€è¡“æ£§

### å¾Œç«¯æ¡†æ¶
- **FastAPI** - ç¾ä»£ã€é«˜æ•ˆèƒ½çš„ Web æ¡†æ¶
- **SQLAlchemy** - ORM è³‡æ–™åº«æ“ä½œ
- **PostgreSQL** - é—œè¯å¼è³‡æ–™åº«

### çˆ¬èŸ²å·¥å…·
- **Selenium** - ç€è¦½å™¨è‡ªå‹•åŒ–ï¼ˆè™•ç† JavaScript ç¶²ç«™ï¼‰
- **BeautifulSoup4** - HTML è§£æ
- **Requests** - HTTP è«‹æ±‚ï¼ˆç°¡å–®ç¶²ç«™ï¼‰

### ä»»å‹™èª¿åº¦
- **APScheduler** - å®šæ™‚ä»»å‹™èª¿åº¦
- **Asyncio** - éåŒæ­¥è™•ç†
- **Multiprocessing** - å¤šé€²ç¨‹åŸ·è¡Œ

### å¯é æ€§
- **Tenacity** - é‡è©¦æ©Ÿåˆ¶
- **Pydantic** - è³‡æ–™é©—è­‰

### éƒ¨ç½²
- **Docker** - å®¹å™¨åŒ–
- **Docker Compose** - å¤šå®¹å™¨ç·¨æ’

---

## è¨­è¨ˆæ¨¡å¼

### 1. ç­–ç•¥æ¨¡å¼ (Strategy Pattern)

æ¯å€‹ Crawler æ˜¯ä¸€å€‹ç­–ç•¥ï¼Œå¯¦ç¾ç›¸åŒçš„ä»‹é¢ï¼š

```python
# æŠ½è±¡ç­–ç•¥
class BaseCrawler(ABC):
    @abstractmethod
    async def crawl_list(self, page: int) -> list:
        """çˆ¬å–æ–‡ç« åˆ—è¡¨"""
        pass

    @abstractmethod
    async def crawl_article(self, url: str) -> dict:
        """çˆ¬å–æ–‡ç« å…§å®¹"""
        pass

# å…·é«”ç­–ç•¥
class LTNCrawler(BaseCrawler):
    async def crawl_list(self, page: int):
        # LTN ç‰¹å®šçš„å¯¦ç¾
        pass

    async def crawl_article(self, url: str):
        # LTN ç‰¹å®šçš„å¯¦ç¾
        pass
```

### 2. å·¥å» æ¨¡å¼ (Factory Pattern)

å‹•æ…‹å‰µå»ºçˆ¬èŸ²å¯¦ä¾‹ï¼š

```python
def get_crawler(crawler_name: str):
    """çˆ¬èŸ²å·¥å» """
    crawlers = {
        'ltn': LTNCrawler(),
        'udn': UDNCrawler(),
        'ettoday': EttodayCrawler(),
        # ...
    }
    return crawlers.get(crawler_name)
```

### 3. æ¨¡æ¿æ–¹æ³•æ¨¡å¼ (Template Method Pattern)

åœ¨ `BaseCrawler` ä¸­å®šç¾©åŸ·è¡Œæµç¨‹ï¼š

```python
class BaseCrawler:
    async def run(self, max_pages, start_date, end_date):
        """æ¨¡æ¿æ–¹æ³•ï¼šå®šç¾©çˆ¬å–æµç¨‹"""
        try:
            self.setup_driver()      # 1. åˆå§‹åŒ–
            articles = []

            for page in range(1, max_pages + 1):
                # 2. çˆ¬å–åˆ—è¡¨ï¼ˆå­é¡å¯¦ç¾ï¼‰
                page_articles = await self.crawl_list(page)

                for article_info in page_articles:
                    # 3. çˆ¬å–å…§å®¹ï¼ˆå­é¡å¯¦ç¾ï¼‰
                    article = await self.crawl_article(article_info)

                    # 4. éæ¿¾ï¼ˆåŸºé¡å¯¦ç¾ï¼‰
                    if self.is_within_date_range(article):
                        articles.append(article)

            return articles
        finally:
            self.cleanup()           # 5. æ¸…ç†
```

### 4. è£é£¾å™¨æ¨¡å¼ (Decorator Pattern)

ç‚ºæ–¹æ³•æ·»åŠ é‡è©¦ã€æ—¥èªŒç­‰åŠŸèƒ½ï¼š

```python
from tenacity import retry

@retry(stop=stop_after_attempt(3))
def wait_and_get(self, url: str):
    """å¸¶é‡è©¦çš„é é¢è¼‰å…¥"""
    self.driver.get(url)
```

---

## é—œéµçµ„ä»¶

### 1. BaseCrawlerï¼ˆåŸºç¤çˆ¬èŸ²é¡ï¼‰

**ä½ç½®**ï¼š`app/services/crawler/base.py`

**è·è²¬**ï¼š
- æä¾› Selenium WebDriver ç®¡ç†
- å¯¦ç¾é‡è©¦æ©Ÿåˆ¶
- æä¾›æ—¥æœŸè§£æå’Œå…§å®¹æ¸…ç†ç­‰å·¥å…·æ–¹æ³•
- å®šç¾©å­é¡å¿…é ˆå¯¦ç¾çš„ä»‹é¢

**æ ¸å¿ƒæ–¹æ³•**ï¼š
```python
class BaseCrawler(ABC):
    # ç”Ÿå‘½é€±æœŸç®¡ç†
    def setup_driver(self)        # åˆå§‹åŒ–ç€è¦½å™¨
    def cleanup(self)              # æ¸…ç†è³‡æº

    # ç¶²é æ“ä½œ
    @retry(...)
    def wait_and_get(self, url)    # è¼‰å…¥é é¢ï¼ˆå¸¶é‡è©¦ï¼‰

    # å·¥å…·æ–¹æ³•
    @staticmethod
    def parse_flexible_date(text)  # è§£æå¤šç¨®æ—¥æœŸæ ¼å¼

    @staticmethod
    def clean_content(content)     # æ¸…ç†æ–‡ç« å…§å®¹

    # æŠ½è±¡æ–¹æ³•ï¼ˆå­é¡å¿…é ˆå¯¦ç¾ï¼‰
    @abstractmethod
    async def crawl_list(page)     # çˆ¬å–åˆ—è¡¨é 

    @abstractmethod
    async def crawl_article(url)   # çˆ¬å–æ–‡ç« å…§å®¹
```

### 2. å…·é«”çˆ¬èŸ²å¯¦ç¾

**ä½ç½®**ï¼š`app/services/crawler/`

æ¯å€‹çˆ¬èŸ²ç¹¼æ‰¿ `BaseCrawler` ä¸¦å¯¦ç¾ç‰¹å®šç¶²ç«™çš„é‚è¼¯ï¼š

```python
class EdgePropCrawler(BaseCrawler):
    def __init__(self):
        super().__init__()
        self.source_name = "EdgeProp Malaysia"
        self.base_url = "https://www.edgeprop.my"

    async def crawl_list(self, page: int) -> List[Dict]:
        """EdgeProp ç‰¹å®šçš„åˆ—è¡¨é è§£æ"""
        url = f"{self.base_url}/news?page={page}"
        self.wait_and_get(url)

        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        articles = soup.select('.article-item')  # EdgeProp ç‰¹å®šé¸æ“‡å™¨

        return [self._parse_article_item(art) for art in articles]

    async def crawl_article(self, article_info: Dict) -> Dict:
        """EdgeProp ç‰¹å®šçš„æ–‡ç« é è§£æ"""
        self.wait_and_get(article_info['url'])

        soup = BeautifulSoup(self.driver.page_source, 'html.parser')

        # ä½¿ç”¨åŸºé¡çš„å·¥å…·æ–¹æ³•
        date = self.parse_flexible_date(
            soup.select_one('.date').text
        )
        content = self.clean_content(
            soup.select_one('.content').text
        )

        return {
            'title': soup.select_one('h1').text,
            'content': content,
            'published_at': date,
            # ...
        }
```

### 3. è³‡æ–™åº«å·¥å…·ï¼ˆDB Utilsï¼‰

**ä½ç½®**ï¼š`app/core/db_utils.py`

**è·è²¬**ï¼š
- æ‰¹æ¬¡æ’å…¥/æ›´æ–°æ“ä½œ
- Upsert é‚è¼¯ï¼ˆå­˜åœ¨å‰‡æ›´æ–°ï¼Œä¸å­˜åœ¨å‰‡æ’å…¥ï¼‰
- è³‡æ–™æ¸…ç†

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
```python
def batch_upsert_articles(
    session: Session,
    articles: List[Dict],
    batch_size: int = 100
) -> tuple[int, int]:
    """
    æ‰¹æ¬¡ Upsert æ“ä½œ

    å„ªé»ï¼š
    - æ¸›å°‘è³‡æ–™åº«é€£æ¥æ¬¡æ•¸
    - ä½¿ç”¨ PostgreSQL çš„ ON CONFLICT èªæ³•
    - è‡ªå‹•è™•ç†æ–°å¢/æ›´æ–°

    Returns:
        (æ–°å¢æ•¸é‡, æ›´æ–°æ•¸é‡)
    """
    for i in range(0, len(articles), batch_size):
        batch = articles[i:i + batch_size]

        for article_data in batch:
            stmt = insert(Article).values(**article_data)
            stmt = stmt.on_conflict_do_update(
                index_elements=['url'],  # URL ä½œç‚ºå”¯ä¸€éµ
                set_={
                    'title': stmt.excluded.title,
                    'content': stmt.excluded.content,
                    # æ›´æ–°æ‰€æœ‰æ¬„ä½...
                }
            )
            session.execute(stmt)

        session.commit()
```

### 4. çˆ¬èŸ²å”èª¿å™¨ï¼ˆOrchestratorï¼‰

**ä½ç½®**ï¼š`app/main.py` ä¸­çš„ `run_crawler_process()`

**è·è²¬**ï¼š
- ç®¡ç†å¤šå€‹çˆ¬èŸ²çš„åŸ·è¡Œ
- æ”¯æ´ä¸¦è¡Œ/ä¸²è¡Œæ¨¡å¼
- ç•°å¸¸éš”é›¢å’Œçµæœçµ±è¨ˆ

**åŸ·è¡Œæµç¨‹**ï¼š
```python
def run_crawler_process(start_date, end_date, parallel=True):
    async def run_single_crawler(source: str):
        """åŸ·è¡Œå–®å€‹çˆ¬èŸ²ï¼ˆå¸¶ç•°å¸¸éš”é›¢ï¼‰"""
        try:
            count = await test_crawler(source, start_date, end_date)
            return {source: {'status': 'success', 'count': count}}
        except Exception as e:
            return {source: {'status': 'failed', 'error': str(e)}}

    async def run():
        sources = ["ltn", "udn", "ettoday", ...]

        if parallel:
            # ä¸¦è¡ŒåŸ·è¡Œï¼ˆå¿« 70%ï¼‰
            tasks = [run_single_crawler(s) for s in sources]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        else:
            # ä¸²è¡ŒåŸ·è¡Œ
            results = []
            for source in sources:
                result = await run_single_crawler(source)
                results.append(result)

        # çµ±è¨ˆçµæœ
        success_count = sum(1 for r in results if r['status'] == 'success')
        logger.info(f"æˆåŠŸ {success_count}/{len(sources)}")

        return results

    asyncio.run(run())
```

### 5. æ’ç¨‹å™¨ï¼ˆSchedulerï¼‰

**ä½ç½®**ï¼š`app/main.py` ä¸­çš„ `setup_scheduler()`

**è·è²¬**ï¼š
- å®šæ™‚è§¸ç™¼çˆ¬èŸ²ä»»å‹™
- ä½¿ç”¨å°ç£æ™‚å€

**è¨­å®š**ï¼š
```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

scheduler = AsyncIOScheduler(timezone=timezone('Asia/Taipei'))

# æ¯å¤© 8:00, 12:00, 16:00, 20:00 åŸ·è¡Œ
for hour in [8, 12, 16, 20]:
    scheduler.add_job(
        crawl_today,
        CronTrigger(hour=hour, minute=0, timezone=timezone('Asia/Taipei')),
        id=f'crawl_{hour}',
        replace_existing=True
    )

scheduler.start()
```

---

## æœ€ä½³å¯¦è¸

### 1. éŒ¯èª¤è™•ç†ä¸‰å±¤é˜²è­·

```python
# ç¬¬ä¸€å±¤ï¼šé‡è©¦æ©Ÿåˆ¶ï¼ˆTenacityï¼‰
@retry(stop=stop_after_attempt(3))
def wait_and_get(self, url):
    self.driver.get(url)

# ç¬¬äºŒå±¤ï¼šç•°å¸¸æ•ç²ï¼ˆå–®å€‹æ–‡ç« å¤±æ•—ï¼‰
async def crawl_article(self, url):
    try:
        # çˆ¬å–é‚è¼¯
        return article_data
    except Exception as e:
        logger.error(f"æ–‡ç« çˆ¬å–å¤±æ•—: {url}")
        return None  # è¿”å› Noneï¼Œç¹¼çºŒä¸‹ä¸€ç¯‡

# ç¬¬ä¸‰å±¤ï¼šç•°å¸¸éš”é›¢ï¼ˆå–®å€‹ Agent å¤±æ•—ï¼‰
async def run_single_crawler(source):
    try:
        count = await test_crawler(source)
        return {'status': 'success', 'count': count}
    except Exception as e:
        return {'status': 'failed', 'error': str(e)}
        # ä¸æ‹‹å‡ºç•°å¸¸ï¼Œè®“å…¶ä»– Agent ç¹¼çºŒåŸ·è¡Œ
```

### 2. è³‡æ–™åº«æ“ä½œå„ªåŒ–

```python
# âŒ ä¸å¥½çš„åšæ³•ï¼šé€æ¢æ’å…¥
for article in articles:
    db.add(Article(**article))
    db.commit()  # æ¯æ¬¡éƒ½æäº¤ï¼Œæ…¢

# âœ… å¥½çš„åšæ³•ï¼šæ‰¹æ¬¡æ“ä½œ
batch_upsert_articles(db, articles, batch_size=50)
# æ¯ 50 æ¢æäº¤ä¸€æ¬¡ï¼Œå¿« 10-20 å€
```

### 3. è³‡æºç®¡ç†

```python
# ä½¿ç”¨ try-finally ç¢ºä¿æ¸…ç†
async def run(self):
    try:
        self.setup_driver()  # åˆå§‹åŒ–è³‡æº
        articles = await self.crawl()
        return articles
    finally:
        self.cleanup()       # ç„¡è«–å¦‚ä½•éƒ½æ¸…ç†
```

### 4. æ—¥æœŸè™•ç†å½ˆæ€§åŒ–

```python
# æ”¯æ´å¤šç¨®æ—¥æœŸæ ¼å¼
date_formats = [
    '%Y-%m-%d %H:%M:%S',  # 2025-01-01 12:00:00
    '%Y/%m/%d',           # 2025/01/01
    '%d %b %Y',           # 01 Jan 2025
    '%B %d, %Y',          # January 01, 2025
    # æ›´å¤šæ ¼å¼...
]

for fmt in date_formats:
    try:
        return datetime.strptime(date_text, fmt)
    except ValueError:
        continue
```

### 5. æ¼¸é€²å¼çˆ¬å–

```python
# å¾æœ€è¿‘çš„æ—¥æœŸé–‹å§‹çˆ¬
# é‡åˆ°å·²å­˜åœ¨çš„æ–‡ç« å°±åœæ­¢ï¼ˆé¿å…é‡è¤‡çˆ¬å–ï¼‰
for page in range(1, max_pages):
    articles = await self.crawl_list(page)

    for article_info in articles:
        article_date = article_info['published_at'].date()

        # æª¢æŸ¥æ—¥æœŸç¯„åœ
        if article_date < start_date:
            return all_articles  # å¤ªèˆŠï¼Œåœæ­¢

        if article_date > end_date:
            continue  # å¤ªæ–°ï¼Œè·³é

        # åœ¨ç¯„åœå…§ï¼Œçˆ¬å–
        article = await self.crawl_article(article_info)
        all_articles.append(article)
```

### 6. åçˆ¬èŸ²å°ç­–

```python
# éš¨æ©Ÿå»¶é²
time.sleep(random.uniform(1, 3))

# å½è£ User-Agent
chrome_options.add_argument(f'user-agent={random_ua}')

# éš±è— WebDriver ç‰¹å¾µ
chrome_options.add_argument('--disable-blink-features=AutomationControlled')
driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
    "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
})

# æ¸…é™¤ Cookies
driver.delete_all_cookies()
```

---

## æ“´å±•æŒ‡å—

### å¦‚ä½•æ·»åŠ æ–°çš„çˆ¬èŸ² Agentï¼Ÿ

**æ­¥é©Ÿ 1ï¼šå‰µå»ºçˆ¬èŸ²é¡**

```python
# app/services/crawler/new_source_crawler.py

from .base import BaseCrawler
from typing import List, Dict, Optional
import logging

logger = logging.getLogger(__name__)

class NewSourceCrawler(BaseCrawler):
    def __init__(self):
        super().__init__()
        self.source_name = "æ–°ä¾†æºåç¨±"
        self.base_url = "https://www.newsource.com"
        # æ ¹æ“šç¶²ç«™éœ€æ±‚è¨­å®š
        self.needs_javascript = True  # å¦‚æœéœ€è¦ JS æ¸²æŸ“

    async def crawl_list(self, page: int = 1) -> List[Dict]:
        """
        çˆ¬å–åˆ—è¡¨é 

        Returns:
            [
                {
                    'title': 'æ–‡ç« æ¨™é¡Œ',
                    'url': 'æ–‡ç« URL',
                    'image_url': 'åœ–ç‰‡URL',
                    'category': 'åˆ†é¡',
                },
                ...
            ]
        """
        url = f"{self.base_url}/news?page={page}"
        self.wait_and_get(url)

        soup = BeautifulSoup(self.driver.page_source, 'html.parser')

        # TODO: æ ¹æ“šç¶²ç«™çµæ§‹èª¿æ•´é¸æ“‡å™¨
        article_items = soup.select('.article-item')

        articles = []
        for item in article_items:
            try:
                title = item.select_one('.title').text.strip()
                url = item.select_one('a')['href']

                # ç¢ºä¿ URL æ˜¯å®Œæ•´çš„
                if not url.startswith('http'):
                    url = self.base_url + url

                articles.append({
                    'title': title,
                    'url': url,
                    'image_url': item.select_one('img')['src'],
                    'category': item.select_one('.category').text.strip(),
                })
            except Exception as e:
                logger.error(f"è§£ææ–‡ç« é …ç›®å¤±æ•—: {str(e)}")
                continue

        return articles

    async def crawl_article(self, article_info: Dict) -> Optional[Dict]:
        """
        çˆ¬å–æ–‡ç« å…§å®¹

        Returns:
            {
                'title': 'æ–‡ç« æ¨™é¡Œ',
                'content': 'æ–‡ç« å…§å®¹',
                'description': 'æ‘˜è¦',
                'published_at': datetime,
                'image_url': 'åœ–ç‰‡URL',
                'category': 'åˆ†é¡',
                'reporter': 'è¨˜è€…',
            }
        """
        url = article_info['url']
        self.wait_and_get(url)

        soup = BeautifulSoup(self.driver.page_source, 'html.parser')

        try:
            # æå–æ¨™é¡Œ
            title = soup.select_one('h1').text.strip()

            # æå–å…§å®¹
            content = soup.select_one('.article-content').text.strip()
            content = self.clean_content(content)  # ä½¿ç”¨åŸºé¡çš„æ¸…ç†æ–¹æ³•

            # æå–æ—¥æœŸï¼ˆä½¿ç”¨åŸºé¡çš„å½ˆæ€§è§£æï¼‰
            date_text = soup.select_one('.publish-date').text.strip()
            published_at = self.parse_flexible_date(date_text)

            # æå–å…¶ä»–è³‡è¨Š
            description = soup.find('meta', {'name': 'description'})
            description = description['content'] if description else content[:200]

            return {
                'title': title,
                'content': content,
                'description': description,
                'published_at': published_at or datetime.now(),
                'image_url': article_info.get('image_url', ''),
                'category': article_info.get('category', ''),
                'reporter': soup.select_one('.author')?.text.strip(),
            }

        except Exception as e:
            logger.error(f"çˆ¬å–æ–‡ç« å…§å®¹å¤±æ•— {url}: {str(e)}")
            return None

    async def crawl(self, start_date=None, end_date=None):
        """
        åŸ·è¡Œçˆ¬èŸ²ï¼ˆå¯é¸å¯¦ç¾ï¼‰

        å¦‚æœç¶²ç«™é‚è¼¯ç‰¹æ®Šï¼Œå¯ä»¥è¦†å¯«é€™å€‹æ–¹æ³•
        å¦å‰‡ä½¿ç”¨ BaseCrawler.run() å³å¯
        """
        try:
            self.setup_driver()

            # è½‰æ›æ—¥æœŸæ ¼å¼
            if isinstance(start_date, str):
                start_date = datetime.strptime(start_date, '%Y-%m-%d').date()
            if isinstance(end_date, str):
                end_date = datetime.strptime(end_date, '%Y-%m-%d').date()

            all_articles = []
            page = 1
            max_pages = 10

            while page <= max_pages:
                articles_list = await self.crawl_list(page)

                if not articles_list:
                    break

                for article_info in articles_list:
                    article_data = await self.crawl_article(article_info)

                    if not article_data:
                        continue

                    # æ—¥æœŸéæ¿¾
                    if start_date and end_date:
                        article_date = article_data['published_at'].date()
                        if not (start_date <= article_date <= end_date):
                            if article_date < start_date:
                                return all_articles  # å¤ªèˆŠï¼Œåœæ­¢
                            continue  # å¤ªæ–°ï¼Œè·³é

                    # åˆä½µè³‡æ–™
                    full_article = {
                        'url': article_info['url'],
                        'source': self.source_name,
                        **article_data
                    }

                    all_articles.append(full_article)

                page += 1

            return all_articles

        finally:
            self.cleanup()
```

**æ­¥é©Ÿ 2ï¼šè¨»å†Šçˆ¬èŸ²**

```python
# app/tests/test_crawler.py

# å°å…¥æ–°çˆ¬èŸ²
from app.services.crawler.new_source_crawler import NewSourceCrawler

# æ·»åŠ åˆ°å·¥å» å‡½æ•¸
def get_crawler(crawler_name: str):
    crawlers = {
        'ltn': LTNCrawler(),
        'udn': UDNCrawler(),
        'newsource': NewSourceCrawler(),  # æ–°å¢é€™è¡Œ
        # ...
    }
    return crawlers.get(crawler_name)

# æ›´æ–°å‘½ä»¤åˆ—åƒæ•¸
parser.add_argument('crawler',
    choices=['ltn', 'udn', 'newsource', ...],  # æ·»åŠ æ–°é¸é …
    help='æŒ‡å®šè¦æ¸¬è©¦çš„çˆ¬èŸ²'
)
```

**æ­¥é©Ÿ 3ï¼šæ·»åŠ åˆ°æ’ç¨‹**

```python
# app/main.py

# å°å…¥æ–°çˆ¬èŸ²
from app.services.crawler.new_source_crawler import NewSourceCrawler

# æ·»åŠ åˆ°ä¾†æºåˆ—è¡¨
def run_crawler_process(start_date, end_date, parallel=True):
    async def run():
        sources = [
            "ltn", "udn", "newsource",  # æ·»åŠ æ–°ä¾†æº
            # ...
        ]
        # ...
```

**æ­¥é©Ÿ 4ï¼šæ¸¬è©¦**

```bash
# æ¸¬è©¦æ–°çˆ¬èŸ²
docker exec reas-web-1 python -m app.tests.test_crawler newsource \
    --start_date 2025-01-01 --end_date 2025-01-03

# å¦‚æœæˆåŠŸï¼Œæœƒçœ‹åˆ°ï¼š
# âœ… newsource çˆ¬èŸ²å®Œæˆï¼Œå…±çˆ¬å– N ç¯‡æ–‡ç« 
```

---

## ç¶“é©—ç¸½çµ

### âœ… æˆåŠŸç¶“é©—

#### 1. æ¨¡çµ„åŒ–è¨­è¨ˆ
- **BaseCrawler** æä¾›çµ±ä¸€ä»‹é¢
- æ¯å€‹çˆ¬èŸ²ç¨ç«‹ï¼Œæ˜“æ–¼ç¶­è­·
- æ–°å¢çˆ¬èŸ²åªéœ€ 3 æ­¥

#### 2. éŒ¯èª¤è™•ç†å±¤æ¬¡åŒ–
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¬¬ä¸‰å±¤ï¼šç•°å¸¸éš”é›¢           â”‚  å–®å€‹ Agent å¤±æ•—ä¸å½±éŸ¿å…¶ä»–
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç¬¬äºŒå±¤ï¼šå–®æ–‡ç« ç•°å¸¸è™•ç†     â”‚  å–®ç¯‡æ–‡ç« å¤±æ•—ç¹¼çºŒä¸‹ä¸€ç¯‡
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç¬¬ä¸€å±¤ï¼šé‡è©¦æ©Ÿåˆ¶           â”‚  ç¶²çµ¡éŒ¯èª¤è‡ªå‹•é‡è©¦
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. æ‰¹æ¬¡æ“ä½œ
- è³‡æ–™åº«æ’å…¥é€Ÿåº¦æå‡ **10-20 å€**
- ä½¿ç”¨ PostgreSQL çš„ `ON CONFLICT` å¯¦ç¾ Upsert
- æ¸›å°‘é€£æ¥é–‹éŠ·

#### 4. ä¸¦è¡ŒåŸ·è¡Œ
- 8 å€‹çˆ¬èŸ²å¾ ~16 åˆ†é˜é™è‡³ ~3-4 åˆ†é˜
- **ç¯€çœ 70-75% æ™‚é–“**
- ä½¿ç”¨ `asyncio.gather()` ç°¡å–®å¯¦ç¾

#### 5. å·¥å…·æ–¹æ³•çµ±ä¸€
- `parse_flexible_date()` - æ”¯æ´ 10+ ç¨®æ—¥æœŸæ ¼å¼
- `clean_content()` - çµ±ä¸€å…§å®¹æ¸…ç†
- æ¸›å°‘ä»£ç¢¼é‡è¤‡ **60%**

### âš ï¸ é‡åˆ°çš„å•é¡Œèˆ‡è§£æ±º

#### å•é¡Œ 1ï¼šJavaScript æ¸²æŸ“çš„ç¶²ç«™
**ç—‡ç‹€**ï¼šçˆ¬å–åˆ°ç©ºç™½å…§å®¹
**åŸå› **ï¼š`--disable-javascript` è¢«å•Ÿç”¨
**è§£æ±º**ï¼š
```python
class BaseCrawler:
    def __init__(self):
        self.needs_javascript = True  # é è¨­å•Ÿç”¨ JS

    def setup_driver(self):
        if not self.needs_javascript:
            chrome_options.add_argument('--disable-javascript')
```

#### å•é¡Œ 2ï¼šè³‡æ–™åº«å¯†ç¢¼ä¸ä¸€è‡´
**ç—‡ç‹€**ï¼šå®¹å™¨å•Ÿå‹•å¤±æ•—ï¼Œèªè­‰éŒ¯èª¤
**åŸå› **ï¼š`.env` æ”¹äº†ä½†æ•¸æ“šå·é‚„æ˜¯èˆŠå¯†ç¢¼
**è§£æ±º**ï¼š
1. åœ¨ `docker-compose.yml` ä¸­æ˜ç¢ºæŒ‡å®šå¯†ç¢¼
2. æˆ–åˆªé™¤æ•¸æ“šå·é‡å»ºï¼š`docker-compose down -v`

#### å•é¡Œ 3ï¼šå–®å€‹çˆ¬èŸ²å¤±æ•—å½±éŸ¿å…¨å±€
**ç—‡ç‹€**ï¼šä¸€å€‹ç¶²ç«™æ›äº†ï¼Œæ‰€æœ‰çˆ¬èŸ²éƒ½åœæ­¢
**åŸå› **ï¼šæ²’æœ‰ç•°å¸¸éš”é›¢
**è§£æ±º**ï¼š
```python
# ä½¿ç”¨ return_exceptions=True
results = await asyncio.gather(
    *tasks,
    return_exceptions=True  # é—œéµï¼
)
```

#### å•é¡Œ 4ï¼šæ—¥æœŸæ ¼å¼ä¸ä¸€è‡´
**ç—‡ç‹€**ï¼šéƒ¨åˆ†æ–‡ç« æ—¥æœŸè§£æå¤±æ•—
**åŸå› **ï¼šæ¯å€‹ç¶²ç«™æ—¥æœŸæ ¼å¼ä¸åŒ
**è§£æ±º**ï¼šå¯¦ç¾å½ˆæ€§æ—¥æœŸè§£æå™¨ï¼Œæ”¯æ´å¤šç¨®æ ¼å¼

#### å•é¡Œ 5ï¼šè¨˜æ†¶é«”å ç”¨éé«˜
**ç—‡ç‹€**ï¼šé•·æ™‚é–“é‹è¡Œå¾Œè¨˜æ†¶é«”ä¸è¶³
**åŸå› **ï¼šChromeDriver æ²’æœ‰æ­£ç¢ºé—œé–‰
**è§£æ±º**ï¼š
```python
def cleanup(self):
    if self.driver:
        try:
            self.driver.quit()
        finally:
            self.driver = None  # ç¢ºä¿é‡‹æ”¾å¼•ç”¨
```

### ğŸ¯ é—œéµæŒ‡æ¨™

| æŒ‡æ¨™ | æ•¸å€¼ | èªªæ˜ |
|------|------|------|
| **çˆ¬èŸ²æ•¸é‡** | 8 å€‹ | å°ç£ 4ã€é¦¬ä¾†è¥¿äº 2ã€é¦™æ¸¯ 2 |
| **å®šæ™‚é »ç‡** | 4 æ¬¡/å¤© | 8am, 12pm, 4pm, 8pm |
| **å¹³å‡æ–‡ç« æ•¸** | 15-20 ç¯‡/ä¾†æº | æ¯æ¬¡çˆ¬å– |
| **æˆåŠŸç‡** | >90% | æœ‰é‡è©¦å’Œå®¹éŒ¯ |
| **åŸ·è¡Œæ™‚é–“** | 3-4 åˆ†é˜ | ä¸¦è¡Œæ¨¡å¼ï¼Œ8 å€‹ä¾†æº |
| **è³‡æ–™åº«å¤§å°** | ~5GB/å¹´ | åŒ…å«å…¨æ–‡å…§å®¹ |

### ğŸ“š æŠ€è¡“æ±ºç­–

#### ç‚ºä»€éº¼é¸æ“‡ Selenium è€Œé Scrapyï¼Ÿ
âœ… **å„ªé»**ï¼š
- è™•ç† JavaScript æ¸²æŸ“çš„ç¶²ç«™
- æ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨è¡Œç‚º
- ç¹éç°¡å–®çš„åçˆ¬èŸ²

âŒ **ç¼ºé»**ï¼š
- è¼ƒæ…¢ï¼ˆéœ€è¦å•Ÿå‹•ç€è¦½å™¨ï¼‰
- è¨˜æ†¶é«”ä½”ç”¨é«˜
- éœ€è¦ ChromeDriver

**çµè«–**ï¼šæœ¬å°ˆæ¡ˆå¾ˆå¤šç¶²ç«™éœ€è¦ JSï¼ŒSelenium æ˜¯åˆé©é¸æ“‡

#### ç‚ºä»€éº¼ç”¨ PostgreSQL è€Œé MongoDBï¼Ÿ
âœ… **å„ªé»**ï¼š
- çµæ§‹åŒ–è³‡æ–™ï¼ˆæ–‡ç« æœ‰å›ºå®šæ¬„ä½ï¼‰
- ACID äº‹å‹™æ”¯æ´
- è±å¯Œçš„æŸ¥è©¢åŠŸèƒ½ï¼ˆå…¨æ–‡æœå°‹ã€æ’åºï¼‰
- `ON CONFLICT` å¯¦ç¾ Upsert

**çµè«–**ï¼šæˆ¿åœ°ç”¢æ–°èæ˜¯çµæ§‹åŒ–è³‡æ–™ï¼Œé—œè¯å¼è³‡æ–™åº«æ›´é©åˆ

#### ç‚ºä»€éº¼ç”¨ FastAPI è€Œé Flaskï¼Ÿ
âœ… **å„ªé»**ï¼š
- åŸç”Ÿæ”¯æ´ async/await
- è‡ªå‹•ç”Ÿæˆ API æ–‡æª”
- Pydantic è³‡æ–™é©—è­‰
- é«˜æ•ˆèƒ½

**çµè«–**ï¼šFastAPI æ›´ç¾ä»£ï¼Œé©åˆéåŒæ­¥çˆ¬èŸ²

---

## æ ¸å¿ƒè¨­è¨ˆåŸå‰‡ç¸½çµ

### 1. **å–®ä¸€è·è²¬åŸå‰‡ï¼ˆSRPï¼‰**
æ¯å€‹çˆ¬èŸ²åªè² è²¬ä¸€å€‹ç¶²ç«™ï¼Œä¸è™•ç†å…¶ä»–é‚è¼¯

### 2. **é–‹æ”¾å°é–‰åŸå‰‡ï¼ˆOCPï¼‰**
å°æ“´å±•é–‹æ”¾ï¼ˆæ˜“æ–¼æ·»åŠ æ–°çˆ¬èŸ²ï¼‰ï¼Œå°ä¿®æ”¹å°é–‰ï¼ˆä¸å½±éŸ¿ç¾æœ‰çˆ¬èŸ²ï¼‰

### 3. **ä¾è³´å€’ç½®åŸå‰‡ï¼ˆDIPï¼‰**
ä¾è³´æŠ½è±¡ï¼ˆBaseCrawlerï¼‰ï¼Œä¸ä¾è³´å…·é«”å¯¦ç¾

### 4. **ä»‹é¢éš”é›¢åŸå‰‡ï¼ˆISPï¼‰**
`BaseCrawler` åªå®šç¾©å¿…è¦çš„ä»‹é¢ï¼ˆ`crawl_list`, `crawl_article`ï¼‰

### 5. **å®¹éŒ¯è¨­è¨ˆ**
å¤±æ•—æ˜¯å¸¸æ…‹ï¼Œç³»çµ±è¦èƒ½å„ªé›…è™•ç†éŒ¯èª¤

### 6. **æ•ˆèƒ½å„ªåŒ–**
æ‰¹æ¬¡æ“ä½œ > é€æ¢æ“ä½œ
ä¸¦è¡ŒåŸ·è¡Œ > ä¸²è¡ŒåŸ·è¡Œ
å¿«å– > é‡è¤‡è¨ˆç®—

---

## æœªä¾†æ“´å±•æ–¹å‘

### çŸ­æœŸï¼ˆ1-2 é€±ï¼‰
- [ ] æ·»åŠ å–®å…ƒæ¸¬è©¦
- [ ] å¯¦ç¾å…§å®¹å»é‡ï¼ˆåŸºæ–¼ç›¸ä¼¼åº¦ï¼‰
- [ ] æ·»åŠ  Prometheus ç›£æ§

### ä¸­æœŸï¼ˆ1-2 æœˆï¼‰
- [ ] å¯¦ç¾æ™ºèƒ½é™æµï¼ˆæ ¹æ“šç¶²ç«™å›æ‡‰èª¿æ•´ï¼‰
- [ ] æ·»åŠ åˆ†ä½ˆå¼çˆ¬å–ï¼ˆCeleryï¼‰
- [ ] å¯¦ç¾å¢é‡çˆ¬å–ï¼ˆåªçˆ¬æ–°æ–‡ç« ï¼‰

### é•·æœŸï¼ˆ3-6 æœˆï¼‰
- [ ] é·ç§»åˆ° Scrapyï¼ˆå¦‚æœéœ€è¦å¤§è¦æ¨¡çˆ¬å–ï¼‰
- [ ] AI å…§å®¹åˆ†é¡å’Œæ‘˜è¦
- [ ] å¯¦æ™‚æ¨é€é€šçŸ¥

---

## é©ç”¨å ´æ™¯

æœ¬æ¶æ§‹é©åˆï¼š
- âœ… å¤šä¾†æºè³‡æ–™èšåˆ
- âœ… å®šæ™‚çˆ¬å–ä»»å‹™
- âœ… éœ€è¦è™•ç† JavaScript çš„ç¶²ç«™
- âœ… ä¸­å°è¦æ¨¡çˆ¬èŸ²å°ˆæ¡ˆï¼ˆ< 100 å€‹ä¾†æºï¼‰

ä¸é©åˆï¼š
- âŒ è¶…å¤§è¦æ¨¡çˆ¬å–ï¼ˆå»ºè­°ç”¨ Scrapy + Distributedï¼‰
- âŒ å¯¦æ™‚çˆ¬å–ï¼ˆéœ€è¦ç”¨ WebSocket æˆ–é•·è¼ªè©¢ï¼‰
- âŒ å°é€Ÿåº¦è¦æ±‚æ¥µé«˜çš„å ´æ™¯ï¼ˆSelenium è¼ƒæ…¢ï¼‰

---

## å¿«é€Ÿé–‹å§‹æ–°å°ˆæ¡ˆ

### 1. è¤‡è£½æ ¸å¿ƒæ–‡ä»¶
```bash
æ–°å°ˆæ¡ˆ/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ services/crawler/
â”‚   â”‚   â”œâ”€â”€ base.py          # è¤‡è£½
â”‚   â”‚   â””â”€â”€ your_crawler.py  # æ–°å»º
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py        # è¤‡è£½ä¸¦ä¿®æ”¹
â”‚   â”‚   â”œâ”€â”€ database.py      # è¤‡è£½
â”‚   â”‚   â”œâ”€â”€ db_utils.py      # è¤‡è£½
â”‚   â”‚   â””â”€â”€ logging_config.py # è¤‡è£½
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ article.py       # æ ¹æ“šéœ€æ±‚ä¿®æ”¹æ¬„ä½
â”‚   â””â”€â”€ main.py              # è¤‡è£½ä¸¦ä¿®æ”¹
â”œâ”€â”€ docker-compose.yml        # è¤‡è£½
â”œâ”€â”€ Dockerfile                # è¤‡è£½
â””â”€â”€ requirements.txt          # è¤‡è£½
```

### 2. ä¿®æ”¹é…ç½®
- æ›´æ–° `config.py` ä¸­çš„å°ˆæ¡ˆåç¨±å’Œè¨­å®š
- æ›´æ–° `Article` æ¨¡å‹æ¬„ä½
- æ›´æ–° `.env` ä¸­çš„è³‡æ–™åº«åç¨±

### 3. å¯¦ç¾çˆ¬èŸ²
- ç¹¼æ‰¿ `BaseCrawler`
- å¯¦ç¾ `crawl_list()` å’Œ `crawl_article()`
- è¨»å†Šåˆ°å·¥å» å‡½æ•¸

### 4. æ¸¬è©¦å’Œéƒ¨ç½²
```bash
docker-compose up --build -d
docker exec <container> python -m app.tests.test_crawler yourcrawler
```

---

## åƒè€ƒè³‡æº

- **FastAPI æ–‡æª”**ï¼šhttps://fastapi.tiangolo.com/
- **Selenium æ–‡æª”**ï¼šhttps://selenium-python.readthedocs.io/
- **SQLAlchemy æ–‡æª”**ï¼šhttps://docs.sqlalchemy.org/
- **Tenacity æ–‡æª”**ï¼šhttps://tenacity.readthedocs.io/
- **APScheduler æ–‡æª”**ï¼šhttps://apscheduler.readthedocs.io/

---

## çµèª

é€™å€‹çˆ¬èŸ²ç³»çµ±çš„æ ¸å¿ƒç†å¿µæ˜¯ï¼š

> **æ¯å€‹æ–°èä¾†æºæ˜¯ä¸€å€‹ç¨ç«‹çš„ Agentï¼Œ
> é€éçµ±ä¸€çš„ä»‹é¢å”ä½œï¼Œ
> åœ¨å®¹éŒ¯çš„ç’°å¢ƒä¸­ä¸¦è¡ŒåŸ·è¡Œï¼Œ
> å°‡è³‡æ–™é«˜æ•ˆåœ°å­˜å…¥è³‡æ–™åº«ã€‚**

å¸Œæœ›é€™ä»½æ–‡æª”èƒ½å¹«åŠ©ä½ å¿«é€Ÿç†è§£ç³»çµ±è¨­è¨ˆï¼Œä¸¦æ‡‰ç”¨åˆ°æ–°çš„çˆ¬èŸ²å°ˆæ¡ˆä¸­ï¼

---

**æ–‡æª”ç‰ˆæœ¬**ï¼šv1.0
**æœ€å¾Œæ›´æ–°**ï¼š2025-11-03
**ä½œè€…**ï¼šClaude Code
**å°ˆæ¡ˆ**ï¼šæˆ¿åœ°ç”¢æ–°èçˆ¬èŸ²ç³»çµ±
